---
title: 'Studying Log: diffusion model'
date: 2024-03-27
permalink: /posts/2024/03-27/blog-post-2/
tags:
  - Studying
  - Method
  - Knowledge
---

写在前面
=============
记录我在diffusion学习过程中的问题和心得，是一篇随时会补充的blog



Concept
=============

![image](https://github.com/xxllff/xu/assets/73295053/ae528456-be13-4500-bd06-a426e1f027d2)

Diffusion有三大主流
1. DDPM（denoising diffusion probabilistic Model）
2. NCSN
3. SDE
![image](https://github.com/xxllff/xu/assets/73295053/8391f7b5-81f6-43a4-8320-80c58cb8994a)

这张图说明了diffusion的前向传播过程（从右到左）以及减噪过程（从左到右）。
![image](https://github.com/xxllff/xu/assets/73295053/b4b9d004-5cba-4a61-9cce-b273f735f0f2)

从DDPM（Denoising Diffusion Probabilistic Model）开始学习
- https://zhuanlan.zhihu.com/p/525106459
- https://learnopencv.com/denoising-diffusion-probabilistic-models/
- https://zhuanlan.zhihu.com/p/612730273

“We can gradually convert one distribution into another using a Markov chain”

![image](https://github.com/xxllff/xu/assets/73295053/066cb3e5-5f07-4d00-8335-c461bc0cbffb)

Figure 1 这张图从左到右展示了diffusion的本质思想：从复杂分布到简单分布（加噪）

Diffusion的流程：首先对data subspace添加随机高斯噪声，经过t步操作使得subspace完全扩散到了整个space【如图2】。再从whole space中进行去噪——实质上是采样。这里的trick是，以whole space为起点进行sample是practicable可实现的，但难点在于如何从全空间中经过t步采样再得到data subspace。

零碎思想记录：“Hence, in the reverse process, we use a deep-learning model at each step to predict the PDF parameters of the forward process.”猜想正向加噪的每步的噪声参数（如高斯噪声中的μ和σ）不同，反向减噪中每步的过程就是用模型去预测这些参数的估计量（μ_θ和σ_θ）。



==================上面这些是DDPM的理论记录=====================

==================接下来的是数学相关推导================
前向过程
 
 
零碎思想记录：“How do we get image xt from xt-1 and how is noise added at each time step? This can be easily understood by using the reparameterization trick in variational autoencoders.”重参数方法用在这里，具体可以看笔记“autoencoder整理”

对图像xt-1可以使用公式(3)进行采样得到xt.
关于βt的取值，DDPM原文中使用了线性的方式：Defined in the range [0.0001, 0.02] and set the total time-steps T = 1000。、
但是这样设计迭代的计算xt使得每次都成需要矩阵乘法t-1次，换了新的计算方法。引入了两个变量如图4.这样的话可以使xt只需要用到x0减少了耗时。【推导见图3】





====================前向推导结束====================

==========接下来是反向的减噪========

前向过程中时间t步中的操作是添加参数为f(β)的高斯噪声，反向过程的目标是寻找一个函数g(.)来“撤销”加噪的过程，这个过程要用神经网络（一般而言）来训练。
W. Feller在1949年证明：高斯和二项式过程的反过程具有和正过程相似的函数表达式。这一点启发可以使用和正向加噪类似的函数构造来实现反向减噪。

“The reverse process is also a Markov chain where a neural network predicts the parameters for the reverse diffusion kernel at each timestep.” ✳

 
 
Figure 5 最终的优化就是对单一参数的优化MSE

 https://arxiv.org/pdf/2209.00796.pdf

Diffusion Models: A Comprehensive Survey of Methods and Applications
2023.10.11
 
Three predominant formulations: 
	denoising diffusion probabilistic models (DDPMs)
	score-based generative models (SGMs)
	stochastic differential equations (Score SDEs)


 
 
文章中这里关于DDPM的原理介绍可以看上面，这里不赘述了。

零散思维记录：在求前项和后向的KL时使用了前向KL（也就是KL(真实分布q|似然分布p)的形式，这种形式可以帮助似然分布p找到真实分布q的均值，另外如果使用后向KL，一般是用来找多值情况，但我猜想DDPM中的扰动使用了各向同性的高斯噪声，这里使用前向KL or 后向KL都可以https://zhuanlan.zhihu.com/p/372835186）


Score-Based Generative Models（SGMs）基于分数的生成模型
“At the core of score-based generative models (Song and Ermon, 2019, 2020) is the concept of (Stein) score (a.k.a., score or score function) ”
NCSN：Noise-conditional score network——用一系列增强的高斯噪声扰乱数据，并通过训练一个深度神经网络模型来联合估计所有噪声数据分布的分数函数噪声水平

朗之万动力学(Langevin dynamics)是物理学中对分子系统进行统计建模(布朗运动)的工具。2011年，Welling & Teh将它与随机梯度下降(SGD)结合为随机梯度朗之万动力学，进行贝叶斯学习 (Bayesian Learning via Stochastic Gradient Langevin Dynamics)。这篇深藏功和名的文章恐怕是最近大火的Diffusion Model的思想根源之一
2020年Stanford的Song Yan提出了基于SGD-Langevin Dynamics-Score-based的生成模型NCSN，NCSN只需要对stein score function（▽x log p(x)）求梯度【注意这里不是对训练参数求梯度，而是对数据点。区别于fisher中的score function】就可以从概率密度p(x)中采样得到样本
 
Figure 6 Langevin Dynamics方程
	为了计算概率p(x;θ),需要利用标准化参数Z（放在分母位置），但很难求【一般都是求和或求积】，但是贝叶斯（对应的Z叫做marginal或evidence（联想到EBOL？））可以利用“比例于∝”，也就是说Z的值不影响结果
	有3种应对计算归一化Z常数的方法：
	近似归一化常数：VAE和EBM使用变分推断(VI) /马尔可夫链蒙特卡洛(MCMC)，缺点是一旦使用这类近似方法，我们就不可能准确的估计概率了，因为概率是通过除以归一化常数得到，而这个值在这里是估计值；
	限制神经网络模型的种类：通过只使用tractable的神经网络模型，比如：自回归模型 (autoregressive)和normalizing flow models，缺点是选择神经网络的灵活性也受到了限制；
	使用GANs：因为GANs对生成过程建模，而不是依赖概率分布，因而完美的绕过了计算归一化常数这个环节，因为GANs不是一个基于概率的模型，因而也很难基于GANs来估计概率。而且GANs的训练难度大容易崩也是广为所知的缺点。
使用score-based方法可以解决
简要介绍Score-based的pipeline
https://zhuanlan.zhihu.com/p/589030280

	引入stein score function（▽x log p(x)）的概念，从而避免了均一化常数Z的影响。举例：对于尚未均一化的PDF pp(xi;θ),有均一化后PDF pz(xi;θ)= p(xi;θ)/Z，利用▽x log pz(xi;θ)= ▽x log p(xi;θ)- ▽x log Z=▽x log p(xi;θ)，消去了Z
	现在得到的stein score可以看作一个表示data分布的向量场，如图7.至此获得了一个函数sθ(x)≈▽x log pz(x;θ)，这个函数可以用于重建数据点的分布。
	为了比较sθ(x)与原始分布▽x log pdata(x;θ)【ground truth】，可以直接将两个向量场放在同一个区间中，利用向量减法获得一种同维的差异向量，对这组向量进行平均化得到的就是差异。用Fisher Divergence来表达 
	但其中ground truth未知，转换计算方法： 
	新问题：trace的引入需要模型多次计算前向和后向，计算消耗大。引入了投影的思想，也就是slice score matching：核心是如果两个高维分布的向量场相似，那么他们投影在一维上的向量场也是相似的
	目前为止，可以得到一个真正的sθ(x)≈▽x log pz(x;θ)，接下来就是如何从sθ(x)采样得到更靠近真实分布的数据点。这里引入Langevin Dynamic，本质是引入噪声使得采样到的数据点不会坍缩成一个值，公式如图6.
	但直接使用结合了LD的score-based效果不好，原因是模型对于低密度的区域学习能力弱（如图7 右 的左上和右下），因此分步对原数据加噪，使得原数据尽可能扩大数据密度同时不影响基本的分布性质


以上是LD+score-based的NCSN的思想，其中噪声的加入是离散的

那么噪声可以连续加入吗？引入SDE
https://zhuanlan.zhihu.com/p/589466244

SDEs：Stochastic Differential Equations——SDE通常包含一个表示随机白噪声的变量，可通过布朗运动的导数得到它，这也是SDE的起源，出现在爱因斯坦的工作中，这些早期的线性SDEs，被称为朗之万方程(Langevin equations)
 
ODE（Ordinary Differential Equation）普通微分方程：
定义： ODE描述一个未知函数的导数与该函数本身之间的关系。它通常涉及到一个独立变量和一个未知函数。
例子： 一个简单的ODE可以是 dy/dx = x，其中y是未知函数，x是独立变量。

SDE（Stochastic Differential Equation）随机微分方程：
定义： SDE是微分方程的一种，它包含了一个随机项，通常表示为dW(t)，其中W(t)是Wiener过程（布朗运动）。这引入了随机性，反映了系统中的不确定性。
例子： 一个简单的SDE可以是 dX(t) = aX(t)dt + bX(t)dW(t)，其中X(t)是未知函数，a和b是常数。

在蛋白质研究中，SDE经常用于建模蛋白质的随机运动，特别是在分子动力学模拟中。这种模型允许考虑到溶液中分子的随机性和蛋白质的不确定性，而不仅仅是确定性的ODE。

应用场景：
ODE应用场景： 适用于描述确定性系统，如物理定律中的运动学和动力学问题。
SDE应用场景： 更适用于描述包含不确定性或随机性的系统，如分子运动、金融市场波动。

数值解法：
ODE数值解法： 常见的数值解法包括Euler方法、Runge-Kutta方法等，用于求解确定性微分方程。
SDE数值解法： 对于SDE，Euler-Maruyama方法等更适合，因为它们能够处理随机项。


 
Figure 8 EM方法处理SDE问题

 

 
Figure 9 变分自编码
变分自编码模型与diffusion model的关系：
	如图Figure 9 变分自编码。
本质：encoder-decoder模型，联系两块的是潜变量z（图中的xT）
推导：input是x，encoder是q(z|x)，decoder是p¬θ(\hat{x}|z)，重点是学习到θ使得pθ可以似然q。引入如变分推断、贝叶斯推断等方法来最大化ELBO（evidence lower bound，也就是p¬θ(\hat{x}|z)的最大下界），
 
Figure 10 ELBO函数
如图Figure 9 ELBO函数 展示，可以看作是一个使用KL散度处理两个分布相似程度的函数KL(pθ|q)。需要最大化ELOB。使用SGD最大化ELOB。
	联系：DDPM可以看做一个具有固定encoder的VAE，其中encoder的过程的每一步可以用 Figure 3 推导过程 计算。将难以计算的ELBO转换为Score-based DDPM可以简化。
 
Figure 11 生成对抗模型
生成对抗模型与diffusion model的关系：
	如图 Figure 10 生成对抗模型。
	本质：一个生成器G（generator）和一个判别器D（discriminator）。实现纳什均衡
	推导：对均衡函数优化，均衡函数是一个关于G和D的max-min函数，如图 Figure 11 均衡函数V(G,D)，
 
Figure 12 均衡函数V(G,D)
目标是获得V(G,D)的鞍点，在鞍点处认为模型实现了纳什均衡、生成器捕获了真实分布的特征。
	联系：GAN自身存在模型训练不稳定的问题，主要是由于G-input和D-input分布不重叠导致不能很好收敛的情况。因此解决方法是在D中加入noise，这一点启发了使用GAN来作为Diffusion model中的noise控制；另一方面GAN可以提高DM中去噪过程的速度。



 
Figure 13 自回归模型
 
Figure 14 标准流形模型
 
Figure 15 基于能量的模型


